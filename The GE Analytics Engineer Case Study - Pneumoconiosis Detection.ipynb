{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The GE Analytics Engineer Case Study - Pneumoconiosis Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "## 1.1 Background\n",
    "\n",
    "A leading hospital wishes to develop a screening program for coal miners, in order to facilitate early detection of Pneumoconiosis. The standard detection procedure involves taking a chest x-ray and examining it for abnormalities that indicate the onset of Pneumoconiosis. A typical doctor’s report divides each lung into three zones (upper, middle and lower) and labels them as normal/abnormal. However, due to the lack of trained doctors with expertise and the large number of patients to be screened, they have requested GE to develop a computer-aided detection system.\n",
    "\n",
    "A team of image analysts have already developed algorithms to segment the lung and divide it into three zones. They have done this for a set of images where the doctor’s labeling for the lung zones is known, and characterized each lung zone in terms of a set of features. Each patient is identified by a unique patient number. The feature data for the various lung zones for each patient, along with the zone label (0=Normal, 1=Abnormal) is given in the attached Excel workbook. The workbook have separate tabs for each lung zone. Let's now import the workbook and see head of each one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd       #data structures and data analysis tools\n",
    "import os                 #operating system interfaces\n",
    "\n",
    "PATH = os.getcwd() + '/input/CollatedPneumoconiosisData-GE Internal.xlsx'       #Linux Path syntax\n",
    "#PATH = os.getcwd() + '\\\\CollatedPneumoconiosisData-GE Internal.xlsx'           #Windows path syntax\n",
    "\n",
    "df_RU = pd.read_excel(io = PATH, sheet_name = 'RightUpper')\n",
    "df_RM = pd.read_excel(io = PATH, sheet_name = 'RightMiddle')\n",
    "df_RL = pd.read_excel(io = PATH, sheet_name = 'RightLower')\n",
    "\n",
    "df_LU = pd.read_excel(io = PATH, sheet_name = 'LeftUpper')\n",
    "df_LM = pd.read_excel(io = PATH, sheet_name = 'LeftMiddle')\n",
    "df_LL = pd.read_excel(io = PATH, sheet_name = 'LeftLower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RU.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LU.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LL.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data tables looks very simialar. First column is Patient ID, then we have numeric continuous values representing different independent variables and last column is label for dependend variable with patient condition. The independent variables are set of features processed from x-ray images by a team. Study instructions provide detailed description of available features.\n",
    "\n",
    "## 1.2 Feature description\n",
    "\n",
    "Each region of interest (lung zone) is characterized in terms of a set of features. Two types of features are extracted in order to describe each region of interest. These are described below:\n",
    "\n",
    "### a) Intensity based\n",
    "A set of 6 features are extracted based on the histogram of intensity values – mean, standard deviation, skewness, kurtosis, energy and entropy. Apart from calculating these on the original ROI, these features are also extracted after applying a difference filter on the image for the purpose of local enhancement. If I(x,y) denotes the image gray value at (x,y), the first and second order filters are defined as:\n",
    "\n",
    "$$L_{1}^{\\theta}(d) = f_{x}cos\\theta + f_{y}sin\\theta$$\n",
    "\n",
    "$$L_{2}^{\\theta}(d) = f_{xx}cos^{2}\\theta + f_{yy}sin^{2}\\theta + f_{x}cos\\theta$$\n",
    "\n",
    "where: \n",
    "- $d$ is the difference scale,\n",
    "- $\\theta$ is the orientation at which the difference is computed,\n",
    "-$f_{x}$ and $f_{y}$ represent the first order difference,\n",
    "- $f_{xx},f_{yy},f_{xy}$ represent the second order difference.\n",
    " \n",
    "First and second order difference filter bank are used with given orientations $\\theta\\in\\{0,30,35,60,90,120,135,150,180\\}$ and given scale $d\\in\\{1,2\\}$. Six intensity-based features are calculated (mean, variance, skewness, kurtosis, energy, entropy) for each filtered image, along with the same features for the raw image without filtering, amounting to a total of 222 features. A subset of 34 features from this set has been provided in the data sheet. These features are labeled with the prefix $Hist\\_d\\_θ$.\n",
    "\n",
    "### b) Co-occurrence matrix based\n",
    "A set of 5 features are also extracted based on the gray level co-occurrence matrix computed for the ROI, namely energy, entropy, local homogeneity, correlation and inertia. The co-occurrence matrix allows to capture the level of similarity and dissimilarity among adjacent pixels in an ROI. Thus, an ROI with an opacity will contain adjacent pixels with similarly high intensities, whereas a normal ROI will not contain such adjacent pixels. Computing these features for various orientations $\\delta=\\{0,45,90,135\\}$ captures this information for various types of adjacency. A subset of 5 of out of 25 such features has been provided in the attached data sheet. These features are labeled with the prefix $CoMatrix\\_Deg\\delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory analysis\n",
    "\n",
    "### 2.1 General overview of the data\n",
    "\n",
    "The input data is stored in different data frame. The first step is to check if we have any empty or missing value and see what are frames shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RU.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RM.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RL.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LU.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LM.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LL.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values in the data frames. Next step is to see what are shapes of data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RU_shape = df_RU.shape\n",
    "RM_shape = df_RM.shape\n",
    "RL_shape = df_RL.shape\n",
    "LU_shape = df_LU.shape\n",
    "LM_shape = df_LM.shape\n",
    "LL_shape = df_LL.shape\n",
    "\n",
    "print(\"Shape of Right Upper zone table is: \", RU_shape)\n",
    "print(\"Shape of Right Middle zone table is: \", RM_shape)\n",
    "print(\"Shape of Right Lower zone table is: \", RL_shape)\n",
    "print(\"Shape of Left Upper zone table is: \", LU_shape)\n",
    "print(\"Shape of Left Middle zone table is: \", LM_shape)\n",
    "print(\"Shape of Left Lower zone table is: \", LL_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every lung zone we have the same number of columns, but the number of rows is different. To examine this let's check if there are any duplicates for patient IDs. We will also plot a histograms to see what is distribution of number of zones examined for each patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ids = df_RU['PatientNumMasked']\n",
    "print('Right Upper data frame has', \n",
    "      len(df_RU[ids.isin(ids[ids.duplicated()])].sort_values('PatientNumMasked')), \n",
    "      'duplicates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = df_RM['PatientNumMasked']\n",
    "print('Right Medium data frame has',\n",
    "      len(df_RM[ids.isin(ids[ids.duplicated()])].sort_values('PatientNumMasked')),\n",
    "      'duplicates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = df_RL['PatientNumMasked']\n",
    "print('Right Lower data frame has', \n",
    "      len(df_RL[ids.isin(ids[ids.duplicated()])].sort_values('PatientNumMasked')), \n",
    "      'duplicates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = df_LU['PatientNumMasked']\n",
    "print('Left Upper data frame has',\n",
    "      len(df_LU[ids.isin(ids[ids.duplicated()])].sort_values('PatientNumMasked')),\n",
    "      'duplicates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = df_LM['PatientNumMasked']\n",
    "print('Left Medium data frame has',\n",
    "      len(df_LM[ids.isin(ids[ids.duplicated()])].sort_values('PatientNumMasked')),\n",
    "      'duplicates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = df_LL['PatientNumMasked']\n",
    "print('Left Lower data frame has',\n",
    "      len(df_LL[ids.isin(ids[ids.duplicated()])].sort_values('PatientNumMasked')),\n",
    "      'duplicates.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicates, so there are patients that did not have exam for every lung zone. Histograms below are showing what is the distribution of data for each patient by lung zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                       #linear algebra\n",
    "import matplotlib.pyplot as plt          #plotting library\n",
    "from matplotlib import gridspec\n",
    "\n",
    "x1 = np.arange(6)\n",
    "xt = ('Left Upper', 'Left Medium', 'Left Lowe',\n",
    "     'Right Upper', 'Right Medium', 'Right Lower')\n",
    "y1 = [LU_shape[0], LM_shape[0], LL_shape[0],\n",
    "      RU_shape[0], RM_shape[0], RL_shape[0]]\n",
    "\n",
    "df_temp = pd.concat([df_RU['PatientNumMasked'], df_RM['PatientNumMasked'], df_RL['PatientNumMasked'], \n",
    "                     df_LU['PatientNumMasked'], df_LM['PatientNumMasked'], df_LL['PatientNumMasked']], axis=0)\n",
    "exam_counts = df_temp.value_counts()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[3, 3]) \n",
    "ax0 = plt.subplot(gs[0])\n",
    "ax0.set_xticklabels(xt, fontdict=None, minor=False, fontsize=14)\n",
    "plt.sca(ax0)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(fontsize=14)\n",
    "ax0.set_title('Number of exams per lung zone', fontsize=20)\n",
    "ax0.bar(x1, y1, color='cornflowerblue', linewidth=0)\n",
    "ax1 = plt.subplot(gs[1])\n",
    "ax1.hist(exam_counts, color='cornflowerblue', linewidth=0)\n",
    "ax1.set_title('Number of examps per patient', fontsize=20)\n",
    "plt.sca(ax1)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "del df_temp            #delete objects that will not be used later\n",
    "del exam_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right and lung zone have similar data distribution. Medium zone of each lung have the most observations, and upper zone have least observations. Majority of patients have full exam of six lung zone.\n",
    "\n",
    "At this point it is important to remember that Pneumoconiosis is labeled for every zone examined for a patient even if only one zone indicate a disease. This can be a problematic when building a model, since healthy zones are classified in training data as ill, when only one zone is ill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important apect of data for building predictive model is balance of data labels. Balanced classes don't need much attantion, while imbalanced classes need to be treated carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fraction of positive classes in Upper Left: ', len(df_LU[df_LU['Label']==1])/len(df_LU))\n",
    "print('Fraction of positive classes in Medium Left: ', len(df_LM[df_LM['Label']==1])/len(df_LM))\n",
    "print('Fraction of positive classes in Lower Left: ', len(df_LL[df_LL['Label']==1])/len(df_LL))\n",
    "print('Fraction of positive classes in Upper Right: ', len(df_RU[df_RU['Label']==1])/len(df_RU))\n",
    "print('Fraction of positive classes in Upper Medium: ', len(df_RM[df_RM['Label']==1])/len(df_RM))\n",
    "print('Fraction of positive classes in Upper Left: ', len(df_RL[df_RL['Label']==1])/len(df_RL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different lung zones yelds slightly different result, overall result is about 35% of positive class. Classes are imbalanced, but imbalance is not big. Depend on final scores, this might need to be adressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Univariate analysis\n",
    "\n",
    "Univariate analysis is first step to get better understanding of independent variables. Provided data contain continuous numerical data. Section below will go into the features one by one. Total of 39 features will be analized to detect any outliers and to compare distributions between two dependent variables, central tendency and spread of the independent variable. Different lung zones will be analized separatelly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_RU['Zone'] = 'Right Upper'\n",
    "df_RM['Zone'] = 'Right Medium'\n",
    "df_RL['Zone'] = 'Right Lower'\n",
    "df_LU['Zone'] = 'Left Upper'\n",
    "df_LM['Zone'] = 'Left Medium'\n",
    "df_LL['Zone'] = 'Left Lower'\n",
    "\n",
    "\n",
    "columns_to_exclude = ['PatientNumMasked', 'Label', 'Zone']\n",
    "for col in df_RU.columns:\n",
    "    if col in columns_to_exclude:\n",
    "        pass\n",
    "    else:\n",
    "        df = pd.concat([df_RU[[col, 'Zone', 'Label']], \n",
    "                        df_RM[[col, 'Zone', 'Label']], \n",
    "                        df_RL[[col, 'Zone', 'Label']],\n",
    "                        df_LU[[col, 'Zone', 'Label']], \n",
    "                        df_LM[[col, 'Zone', 'Label']], \n",
    "                        df_LL[[col, 'Zone', 'Label']]], axis=0)\n",
    "\n",
    "        fig = plt.figure(figsize=(16, 5))\n",
    "        ax = plt.axes()\n",
    "        plt.hold(True)\n",
    "\n",
    "        bp = sns.boxplot(x='Zone', y=col, hue='Label', data=df, palette='RdBu_r')\n",
    "        plt.title('Boxplots of feature '+col+' by lung zones.', fontsize=20)\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        plt.axes().get_yaxis().get_label().set_visible(False)\n",
    "        plt.axes().get_xaxis().get_label().set_visible(False)\n",
    "        L=plt.legend(fontsize=14, loc=2)\n",
    "        L.get_texts()[0].set_text('Healthy')\n",
    "        L.get_texts()[1].set_text('Sick')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate analysis revealed many outliers in the dataset. Worth to mention is that extreem outliers occurs only in 'Healthy' class. Some of features contains very sevire outliers (ex. Hist_2_180_2_Kurtosis Right Lower zone). These outliers will need to be handeled in later part. During the analysis differences between class labels are visible for some features (ex. Hist_2_135_1_Entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Multivariate analysis\n",
    "\n",
    "Multivariate is second step, it is used to see relationship between features. Because dataset contains only continuous features, during the analysis we will only compare these type of features with breakdown into class labels to get additional information.\n",
    "\n",
    "Comparing every feature will result in 39x39 grid which is too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dict_zones = {'Right Upper': df_RU, 'Right Medium': df_RM, 'Righ Lower': df_RL,\n",
    "             'Left Upper': df_LU, 'Left Medium': df_LM, 'Left Lower': df_LL}\n",
    "columns_all = df_RU.columns.tolist()         #columns in every dataframe are the same\n",
    "columns_keep = set(columns_all) - set(columns_to_exclude)\n",
    "\n",
    "for key, value in dict_zones.items():\n",
    "    df_hm = value[list(columns_keep)]\n",
    "    cor = df_hm.corr()               #Pearson correlation coefficients\n",
    "      \n",
    "    fig = plt.figure(figsize=(30, 30))\n",
    "    hm = sns.heatmap(cor, annot=True, fmt=\".2f\", cmap='RdBu_r')\n",
    "    plt.title('Heatmap of features for '+key+' lung zone', fontsize=35)\n",
    "    plt.xticks(fontsize=25)\n",
    "    plt.yticks(fontsize=25)\n",
    "    cax = plt.gcf().axes[-1]\n",
    "    cax.tick_params(labelsize=25)\n",
    "    plt.show()\n",
    "    \n",
    "    cor = df_hm.corr().abs()\n",
    "    mask = np.zeros_like(cor)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    cor = cor*mask\n",
    "    np.fill_diagonal(cor.values, -2)\n",
    "    s = cor.unstack()\n",
    "    s= s.sort_values(ascending=False)\n",
    "    df_corr = pd.DataFrame(s, columns=['Correlation'])\n",
    "    df_corr = df_corr.sort_values(by='Correlation', ascending=False)\n",
    "    df_corr = df_corr[df_corr['Correlation']>0.7]\n",
    "    print('Table of features Pearson correlation coefficients (corr > 0.7) for ',key,' lung zone\\n')\n",
    "    print(df_corr)\n",
    "\n",
    "del dict_zones\n",
    "del df_hm\n",
    "\n",
    "###### Would be good to fix range for colorbar between heatmaps #######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building model and predictions\n",
    "\n",
    "\n",
    "\n",
    "### 3.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Patient ids were removed to avoid data leakage\n",
    "\n",
    "X_col = ['Hist_0_0_0_Mean', 'Hist_0_0_0_Skewness',\n",
    "       'Hist_0_0_0_Kurtosis', 'Hist_0_0_0_Entropy', 'Hist_2_45_1_Entropy',\n",
    "       'Hist_2_60_1_Skewness', 'Hist_2_90_1_Skewness', 'Hist_2_90_1_Kurtosis',\n",
    "       'Hist_2_135_1_Entropy', 'Hist_1_150_1_Skewness',\n",
    "       'Hist_2_180_1_Skewness', 'Hist_1_30_2_Mean', 'Hist_2_30_2_Mean',\n",
    "       'Hist_2_30_2_Entropy', 'Hist_2_60_2_Skewness', 'Hist_2_60_2_Kurtosis',\n",
    "       'Hist_1_90_2_Skewness', 'Hist_2_90_2_Mean', 'Hist_2_90_2_Skewness',\n",
    "       'Hist_2_90_2_Kurtosis', 'Hist_1_120_2_Mean', 'Hist_1_135_2_Mean',\n",
    "       'Hist_1_135_2_Entropy', 'Hist_2_150_2_Mean', 'Hist_2_150_2_Skewness',\n",
    "       'Hist_2_150_2_Kurtosis', 'Hist_2_150_2_Entropy', 'Hist_1_180_2_Mean',\n",
    "       'Hist_1_180_2_StdDev', 'Hist_1_180_2_Skewness', 'Hist_2_180_2_Mean',\n",
    "       'Hist_2_180_2_Skewness', 'Hist_2_180_2_Kurtosis',\n",
    "       'Hist_2_180_2_Entropy', 'CoMatrix_Deg45_Local_Homogeneity',\n",
    "       'CoMatrix_Deg90_Local_Homogeneity', 'CoMatrix_Deg135_Local_Homogeneity',\n",
    "       'CoMatrix_Deg135_Correlation', 'CoMatrix_Deg135_Inertia']\n",
    "y_col = ['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_outliers(df, columns):\n",
    "    from tqdm import tqdm_notebook\n",
    "    for col in tqdm_notebook(df[columns].columns, total=len(df[columns].columns)):\n",
    "        p25, p50, p75 = np.percentile(df[col], 0.25), np.percentile(df[col], 0.5), np.percentile(df[col], 0.75)\n",
    "        iqr = p75 - p25\n",
    "        #Labels 1\n",
    "        df.loc[df.Label == 1, col] = np.where(df[df['Label']==1][col] > p75 + 100*iqr, p50, df[df['Label']==1][col])\n",
    "        df.loc[df.Label == 1, col] = np.where(df[df['Label']==1][col] < p25 - 100*iqr, p50, df[df['Label']==1][col])\n",
    "        #Labels0\n",
    "        df.loc[df.Label == 0, col] = np.where(df[df['Label']==0][col] > p75 + 100*iqr, p50, df[df['Label']==0][col])\n",
    "        df.loc[df.Label == 0, col] = np.where(df[df['Label']==0][col] < p25 - 100*iqr, p50, df[df['Label']==0][col])        \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43cecc3cdbd4e5a9afbaeda0eadf44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c24100b9a14dd5a1bb213116e5d812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db944db047b4aa0a6993267387ad836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff79910bacc0492fb416d6acf79b7416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98963d9a55c64ed28a5affbac9e113cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8aafb3a5020446f911de6edb0e5a50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Outliers reduction has meaningfull impact on score\n",
    "import numpy as np\n",
    "df_RU = treat_outliers(df_RU, X_col)\n",
    "df_RM = treat_outliers(df_RM, X_col)\n",
    "df_RL = treat_outliers(df_RL, X_col)\n",
    "\n",
    "df_LU = treat_outliers(df_LU, X_col)\n",
    "df_LM = treat_outliers(df_LM, X_col)\n",
    "df_LL = treat_outliers(df_LL, X_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_RU = df_RU[X_col]\n",
    "X_RM = df_RM[X_col]\n",
    "X_RL = df_RL[X_col]\n",
    "X_LU = df_LU[X_col]\n",
    "X_LM = df_LM[X_col]\n",
    "X_LL = df_LL[X_col]\n",
    "\n",
    "y_RU = df_RU[y_col]\n",
    "y_RM = df_RM[y_col]\n",
    "y_RL = df_RL[y_col]\n",
    "y_LU = df_LU[y_col]\n",
    "y_LM = df_LM[y_col]\n",
    "y_LL = df_LL[y_col]\n",
    "\n",
    "RU_ids = df_RU['PatientNumMasked'].values\n",
    "RM_ids = df_RM['PatientNumMasked'].values\n",
    "RL_ids = df_RL['PatientNumMasked'].values\n",
    "LU_ids = df_LU['PatientNumMasked'].values\n",
    "LM_ids = df_LM['PatientNumMasked'].values\n",
    "LL_ids = df_LL['PatientNumMasked'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=sklearn.cross_validation.LeaveOneOut(n=434),\n",
       "       error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params={}, iid=True, n_jobs=1, param_grid={},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='recall', verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cross_validation import LeaveOneOut\n",
    "from sklearn.metrics import recall_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(\n",
    "#    df_LL[X_col], df_LL[y_col], test_size=0.33, random_state=42)\n",
    "\n",
    "#grid_values = {'n_jobs':[4],\n",
    "#              'objective':['binary:logistic'],\n",
    "#              'learning_rate': np.arange(0.01, 0.1, 0.01),\n",
    "#               'max_depth': np.arange(1, 6, 1),\n",
    "#               'n_estimators': np.arange(100, 1000, 100),\n",
    "#               'gamma': np.arange(0, 0.05, 0.01),\n",
    "#               'max_delta_step': np.arange(0, 0.05, 0.01),\n",
    "#               'reg_alpha': np.arange(0, 0.5, 0.1),\n",
    "#               'reg_lambda': np.arange(0.5, 1, 0.1),\n",
    "#               'scale_pos_weight': np.arange(0.7, 1.3, 0.1),\n",
    "#               'random_state': [8]\n",
    "\n",
    "#grid_values = {'n_jobs': [4],\n",
    "#              'objective':['binary:logistic'],\n",
    "#              'learning_rate': np.arange(0.01, 0.1, 0.01),\n",
    "#               'max_depth': np.arange(1, 6, 1),              \n",
    "#              }\n",
    "\n",
    "grid_values = {}\n",
    "              \n",
    "loo = LeaveOneOut(len(df_LL[y_col]))\n",
    "111\n",
    "clf_xgb = XGBClassifier()\n",
    "grid_clf_recall = GridSearchCV(clf_xgb, grid_values, scoring='recall',cv=loo)\n",
    "grid_clf_recall.fit(df_LL[X_col], df_LL[y_col])\n",
    "\n",
    "#y_xgb_recall = grid_clf_recall.decision_function(X_test)\n",
    "#print('Best Recall: ', recall_score(y_test, y_xgb_recall))\n",
    "#print('Best parameters: ', grid_clf_recall.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "0.29723502304147464\n"
     ]
    }
   ],
   "source": [
    "best_parameters, score, _ = max(grid_clf_recall.grid_scores_, key=lambda x: x[1])\n",
    "print(best_parameters)\n",
    "print(score)\n",
    "#{'n_estimators': 600, 'gamma': 0, 'max_depth': 2, \n",
    "#'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.07, 'max_delta_step': 0.0}\n",
    "#{'max_depth': 4, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.22811, std: 0.41961, params: {'max_depth': 1, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.01},\n",
       " mean: 0.25115, std: 0.43368, params: {'max_depth': 2, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.01},\n",
       " mean: 0.25346, std: 0.43499, params: {'max_depth': 3, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.01},\n",
       " mean: 0.27419, std: 0.44611, params: {'max_depth': 4, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.01},\n",
       " mean: 0.26959, std: 0.44374, params: {'max_depth': 5, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.01},\n",
       " mean: 0.24194, std: 0.42826, params: {'max_depth': 1, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.02},\n",
       " mean: 0.26498, std: 0.44132, params: {'max_depth': 2, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.02},\n",
       " mean: 0.27189, std: 0.44493, params: {'max_depth': 3, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.02},\n",
       " mean: 0.28802, std: 0.45284, params: {'max_depth': 4, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.02},\n",
       " mean: 0.27880, std: 0.44841, params: {'max_depth': 5, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.02},\n",
       " mean: 0.25115, std: 0.43368, params: {'max_depth': 1, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.03},\n",
       " mean: 0.27419, std: 0.44611, params: {'max_depth': 2, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.03},\n",
       " mean: 0.29263, std: 0.45497, params: {'max_depth': 3, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.03},\n",
       " mean: 0.29032, std: 0.45391, params: {'max_depth': 4, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.03},\n",
       " mean: 0.28571, std: 0.45175, params: {'max_depth': 5, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.03},\n",
       " mean: 0.26498, std: 0.44132, params: {'max_depth': 1, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.04},\n",
       " mean: 0.28571, std: 0.45175, params: {'max_depth': 2, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.04},\n",
       " mean: 0.29263, std: 0.45497, params: {'max_depth': 3, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.04},\n",
       " mean: 0.29954, std: 0.45806, params: {'max_depth': 4, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.04},\n",
       " mean: 0.28802, std: 0.45284, params: {'max_depth': 5, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.04},\n",
       " mean: 0.27419, std: 0.44611, params: {'max_depth': 1, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.05},\n",
       " mean: 0.29493, std: 0.45601, params: {'max_depth': 2, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.05},\n",
       " mean: 0.29724, std: 0.45704, params: {'max_depth': 3, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.05},\n",
       " mean: 0.30415, std: 0.46005, params: {'max_depth': 4, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.05},\n",
       " mean: 0.29263, std: 0.45497, params: {'max_depth': 5, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.05},\n",
       " mean: 0.27650, std: 0.44727, params: {'max_depth': 1, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.060000000000000005},\n",
       " mean: 0.29954, std: 0.45806, params: {'max_depth': 2, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.060000000000000005},\n",
       " mean: 0.29724, std: 0.45704, params: {'max_depth': 3, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.060000000000000005},\n",
       " mean: 0.30184, std: 0.45906, params: {'max_depth': 4, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.060000000000000005},\n",
       " mean: 0.29724, std: 0.45704, params: {'max_depth': 5, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.060000000000000005},\n",
       " mean: 0.28341, std: 0.45065, params: {'max_depth': 1, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.06999999999999999},\n",
       " mean: 0.30415, std: 0.46005, params: {'max_depth': 2, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.06999999999999999},\n",
       " mean: 0.29954, std: 0.45806, params: {'max_depth': 3, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.06999999999999999},\n",
       " mean: 0.30184, std: 0.45906, params: {'max_depth': 4, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.06999999999999999},\n",
       " mean: 0.29724, std: 0.45704, params: {'max_depth': 5, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.06999999999999999},\n",
       " mean: 0.29493, std: 0.45601, params: {'max_depth': 1, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.08},\n",
       " mean: 0.30415, std: 0.46005, params: {'max_depth': 2, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.08},\n",
       " mean: 0.29954, std: 0.45806, params: {'max_depth': 3, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.08},\n",
       " mean: 0.29724, std: 0.45704, params: {'max_depth': 4, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.08},\n",
       " mean: 0.29493, std: 0.45601, params: {'max_depth': 5, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.08},\n",
       " mean: 0.29493, std: 0.45601, params: {'max_depth': 1, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.09},\n",
       " mean: 0.30184, std: 0.45906, params: {'max_depth': 2, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.09},\n",
       " mean: 0.29724, std: 0.45704, params: {'max_depth': 3, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.09},\n",
       " mean: 0.29724, std: 0.45704, params: {'max_depth': 4, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.09},\n",
       " mean: 0.29724, std: 0.45704, params: {'max_depth': 5, 'n_jobs': 4, 'objective': 'binary:logistic', 'learning_rate': 0.09}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf_recall.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "clf_rf = RandomForestClassifier(n_estimators=10, criterion='gini', min_samples_split=2, \n",
    "                                min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "                                min_impurity_decrease=0.0, bootstrap=True)\n",
    "\n",
    "predictions_rf = []\n",
    "\n",
    "for train, test in tqdm_notebook(loo.split(X), total=len(y)):\n",
    "    one = y.loc[test].index\n",
    "    rest = y.loc[train].index\n",
    "\n",
    "    X_test_loo = X.loc[one]\n",
    "    X_train_loo = X.loc[rest]\n",
    "    y_test_loo = y.loc[one]\n",
    "    y_train_loo = y.loc[rest]\n",
    "    \n",
    "    clf_rf.fit(X_train_loo, y_train_loo.values.ravel())\n",
    "    predictions_rf.append(clf_rf.predict(X_test_loo)[0])\n",
    "    \n",
    "predictions_rf = np.array(predictions_rf).reshape(len(y),1)    \n",
    "    \n",
    "print('Precission score: ', precision_score(y, predictions_rf))\n",
    "print('Accuracy score: ', accuracy_score(y, predictions_rf))\n",
    "print('Recall score: ', recall_score(y, predictions_rf))\n",
    "\n",
    "#Default\n",
    "#Precission score:  0.9108910891089109\n",
    "#Accuracy score:  0.9183673469387755\n",
    "#Recall score:  0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_predictions(X, y):\n",
    "    import xgboost\n",
    "    from sklearn.model_selection import LeaveOneOut\n",
    "    from tqdm import tqdm_notebook\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    clf_xgb = xgboost.XGBClassifier(learning_rate =0.1,\n",
    "                                     n_estimators=500,\n",
    "                                     max_depth=5,\n",
    "                                     min_child_weight=1,\n",
    "                                     gamma=0,\n",
    "                                     subsample=0.8,\n",
    "                                     colsample_bytree=0.8,\n",
    "                                     objective= 'binary:logistic',\n",
    "                                     nthread=4,\n",
    "                                     scale_pos_weight=1,\n",
    "                                     seed=27)\n",
    "\n",
    "    predictions_xgb = []\n",
    "\n",
    "    for train, test in tqdm_notebook(loo.split(X), total=len(y)):\n",
    "        one = y.loc[test].index\n",
    "        rest = y.loc[train].index\n",
    "\n",
    "        X_test_loo = X.loc[one]\n",
    "        X_train_loo = X.loc[rest]\n",
    "        y_test_loo = y.loc[one]\n",
    "        y_train_loo = y.loc[rest]\n",
    "        clf_xgb.fit(X_train_loo, y_train_loo.values.ravel())\n",
    "        predictions_xgb.append(clf_xgb.predict(X_test_loo)[0])\n",
    "    \n",
    "    return np.array(predictions_xgb).reshape(len(y),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_RU = xgb_predictions(X_RU, y_RU).reshape((len(y_RU),))\n",
    "predictions_RM = xgb_predictions(X_RM, y_RM).reshape((len(y_RM),))\n",
    "predictions_RL = xgb_predictions(X_RL, y_RL).reshape((len(y_RL),))\n",
    "predictions_LR = xgb_predictions(X_LU, y_LU).reshape((len(y_LU),))\n",
    "predictions_LM = xgb_predictions(X_LM, y_LM).reshape((len(y_LM),))\n",
    "predictions_LL = xgb_predictions(X_LL, y_LL).reshape((len(y_LL),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_RU = predictions_RU.reshape((len(y_RU),))\n",
    "predictions_RM = predictions_RM.reshape((len(y_RM),))\n",
    "predictions_RL = predictions_RL.reshape((len(y_RL),))\n",
    "predictions_LU = predictions_LR.reshape((len(y_LU),))\n",
    "predictions_LM = predictions_LM.reshape((len(y_LM),))\n",
    "predictions_LL = predictions_LL.reshape((len(y_LL),))\n",
    "\n",
    "y_RU = y_RU.values.reshape((len(y_RU),))\n",
    "y_RM = y_RM.values.reshape((len(y_RM),))\n",
    "y_RL = y_RL.values.reshape((len(y_RL),))\n",
    "y_LU = y_LU.values.reshape((len(y_LU),))\n",
    "y_LM = y_LM.values.reshape((len(y_LM),))\n",
    "y_LL = y_LL.values.reshape((len(y_LL),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RU_pred = pd.DataFrame({'y_pred_RU': predictions_RU, 'y_true_RU': y_RU}, index=RU_ids)\n",
    "df_RM_pred = pd.DataFrame({'y_pred_RM': predictions_RM, 'y_true_RM': y_RM}, index=RM_ids)\n",
    "df_RL_pred = pd.DataFrame({'y_pred_RL': predictions_RL, 'y_true_RL': y_RL}, index=RL_ids)\n",
    "df_LU_pred = pd.DataFrame({'y_pred_LU': predictions_LU, 'y_true_LU': y_LU}, index=LU_ids)\n",
    "df_LM_pred = pd.DataFrame({'y_pred_LM': predictions_LM, 'y_true_LM': y_LM}, index=LM_ids)\n",
    "df_LL_pred = pd.DataFrame({'y_pred_LL': predictions_LL, 'y_true_LL': y_LL}, index=LL_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ensemble = df_RU_pred.merge(df_RM_pred, how='outer', right_index=True, left_index=True)\n",
    "df_ensemble = df_ensemble.merge(df_RL_pred, how='outer', right_index=True, left_index=True)\n",
    "df_ensemble = df_ensemble.merge(df_LU_pred, how='outer', right_index=True, left_index=True)\n",
    "df_ensemble = df_ensemble.merge(df_LM_pred, how='outer', right_index=True, left_index=True)\n",
    "df_ensemble = df_ensemble.merge(df_LL_pred, how='outer', right_index=True, left_index=True)\n",
    "df_ensemble['y_pred'] = np.sum(df_ensemble[['y_pred_RU','y_pred_RM','y_pred_RL',\n",
    "                                        'y_pred_LU', 'y_pred_LM', 'y_pred_LL']], axis=1)\n",
    "df_ensemble['y_true'] = np.sum(df_ensemble[['y_true_RU','y_true_RM','y_true_RL',\n",
    "                                        'y_true_LU', 'y_true_LM', 'y_true_LL']], axis=1)\n",
    "\n",
    "df_ensemble.loc[df_ensemble['y_pred'] > 0, 'y_pred'] = 1\n",
    "df_ensemble.loc[df_ensemble['y_true'] > 0, 'y_true'] = 1\n",
    "df_ensemble[(df_ensemble['y_true']==1) & (df_ensemble['y_pred']==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print('Precission score: ',precision_score(df_ensemble['y_true'], df_ensemble['y_pred']))\n",
    "print('Accuracy score: ', accuracy_score(df_ensemble['y_true'], df_ensemble['y_pred']))\n",
    "print('Recall score: ', recall_score(df_ensemble['y_true'], df_ensemble['y_pred']))\n",
    "\n",
    "#First shot\n",
    "#Precission score:  0.8\n",
    "#Accuracy score:  0.8710359408033826\n",
    "#Recall score:  0.9183673469387755"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ideas: \n",
    "- heatmap: add table with feature pairs that corr>0.7\n",
    "- Dimmensionality reduction: https://en.wikipedia.org/wiki/Principal_component_analysis \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sections:\n",
    "- Introduction\n",
    "- methods\n",
    "- results\n",
    "- conclusions\n",
    "\n",
    "include discussion of the data used, exploratory analysis, model type(s) identification, estimation, diagnosis, validation, and recomendations. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
